{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model free methods: Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gynmasium environments\n",
    "[Farama Gym](https://gymnasium.farama.org/index.html) is a collection of environments designed for developing and comparing reinforcement learning algorithms. It is a Python library that offers easy-to-use interfaces for a wide range of environments, originally provided by OpenAI. The environments are organized into categories such as classic control, Box2D, toy text, algorithmic, MuJoCo, robotics, and more.\n",
    "\n",
    "Each environment is implemented as a Python class that provides consistent methods to interact with them. These methods include:\n",
    "\n",
    "- `reset()`: Resets the environment to its initial state and returns the initial `observation`.\n",
    "- `step(action)`: Executes the provided action, returning the next `observation`, the `reward`, two termination booleans (`truncated` or `terminated`), and an additional `info` dictionary.\n",
    "- `render()`: Renders the environment for visualization purposes.\n",
    "- `close()`: Closes the environment and frees resources.\n",
    "- `seed(seed_value)`: Sets the seed for the environment to ensure reproducible results.\n",
    "\n",
    "This standardized interface makes it simple to develop, test, and compare RL algorithms across different environments.\n",
    "\n",
    "\n",
    "### FrozenLake-v1\n",
    "The FrozenLake-v1 environment is a 4x4 grid world where the agent has to reach the goal without falling into a hole. The agent can move in four directions: up, down, left, and right. The environment is stochastic, meaning that the agent can slip and move in a different direction than the one it chose. The environment is considered solved when the agent reaches the goal state. The agent receives a reward of 1 when it reaches the goal and 0 otherwise.\n",
    "\n",
    "<img src=\"https://gymnasium.farama.org/_images/frozen_lake.gif\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 different versions of the FrozenLake environment are proposed:\n",
    "\n",
    "* A deterministic one\n",
    "* A stochastic one with only 2 holes\n",
    "* A stochastic one with 4 holes\n",
    "* A stochastic one with an 8x8 map\n",
    "\n",
    "Your implementation should be able to solve always the deterministic one and the others around 50% of the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "# env = gym.make('FrozenLake-v1', desc=[\"SFFF\", \"FHFH\", \"FFFH\", \"HFFG\"],  map_name=\"4x4\", is_slippery=False, render_mode=\"rgb_array\") # --> Deterministic (no slippery), Easy\n",
    "# env = gym.make('FrozenLake-v1', desc=[\"SFFH\", \"FFFF\", \"FFFF\", \"HFFG\"],  map_name=\"4x4\", is_slippery=True, render_mode=\"rgb_array\") # --> Stochastic (slippery), More Challenging\n",
    "env = gym.make('FrozenLake-v1', desc=[\"SFFF\", \"FHFH\", \"FFFH\", \"HFFG\"],  map_name=\"4x4\", is_slippery=True, render_mode=\"rgb_array\") # --> Very Challenging\n",
    "# env = gym.make('FrozenLake-v1', desc=[\"SFFFFFFF\", \"FFFFFFFF\", \"HHFFFFFF\", \"HHFFFFFF\", \"HHFFFFFF\", \"HHFFFFFF\", \"HHFFFFFF\", \"FFGFFFFF\"],  map_name=\"8x8\", is_slippery=True, render_mode=\"rgb_array\") # --> Very Challenging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q algorithm Implementation\n",
    "\n",
    "The Q-learning algorithm is a model-free reinforcement learning algorithm that learns by interacting with the environment. The algorithm learns by sampling episodes and updating the value function based on the returns obtained. The Q-learning algorithm is an off-policy algorithm, meaning that it learns the optimal target policy following a different epsilon-greedy policy to generate the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Epsilon Greeedy policy\n",
    "\n",
    "Implement a function that given a Q table and a state, and and epsilon value returns the action to take following an epsilon-greedy policy. The epsilon-greedy policy selects a random action with probability epsilon and the action with the highest Q-value with probability 1-epsilon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def epsilon_greedy_policy(Q, state, epsilon, n_actions):\n",
    "    \"\"\"\n",
    "    Choose an action using the epsilon-greedy strategy.\n",
    "    Args:\n",
    "        Q: The Q-table\n",
    "        state: The current state\n",
    "        epsilon: The exploration rate\n",
    "        n_actions: Total number of actions available\n",
    "    Returns:\n",
    "        action: The action selected\n",
    "    \"\"\"\n",
    "    if np.random.rand() < (1 - epsilon):\n",
    "        action = np.argmax(Q[state, :])\n",
    "    else:\n",
    "        action = np.random.choice(range(n_actions))\n",
    "\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Q-learning\n",
    "\n",
    "Implement the Q-learning algorithm to learn the optimal Q-values for the FrozenLake environment. The Q-learning algorithm learns the optimal Q-values by sampling episodes and updating the Q-values based on the returns obtained. The Q-values are updated using the following formula:\n",
    "\n",
    "$$Q(s, a) = Q(s, a) + \\alpha \\left( r + \\gamma \\max_{a'} Q(s', a') - Q(s, a) \\right)$$\n",
    "\n",
    "where:\n",
    "- $Q(s, a)$ is the Q-value of state $s$ and action $a$.\n",
    "- $\\alpha$ is the learning rate.\n",
    "- $r$ is the reward obtained after taking action $a$ in state $s$.\n",
    "- $\\gamma$ is the discount factor.\n",
    "- $s'$ is the next state.\n",
    "- $a'$ is the next action.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(env, episodes, alpha=0.1, gamma=0.9, epsilon=0.3):\n",
    "    \"\"\"\n",
    "    Q-Learning algorithm implementation.\n",
    "    Args:\n",
    "        env: The environment\n",
    "        episodes: The number of episodes to train for\n",
    "        alpha: The learning rate\n",
    "        gamma: The discount factor\n",
    "        epsilon: The exploration rate\n",
    "    Returns:\n",
    "        Q: The learned Q-table\n",
    "        policy: The learned policy\n",
    "    \"\"\"\n",
    "    Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "    policy = np.zeros(env.observation_space.n, dtype=int)\n",
    "    for i in range(episodes):\n",
    "        obs, _ = env.reset()\n",
    "        state = obs\n",
    "        while True:\n",
    "            action = epsilon_greedy_policy(Q, state, epsilon, env.action_space.n)\n",
    "            state_prime, reward, terminated, truncated, info = env.step(action)\n",
    "            Q[state, action] = Q[state, action] + alpha * (reward + gamma * np.max(Q[state_prime, :]) - Q[state, action])\n",
    "            policy[state] = np.argmax(Q[state, :])\n",
    "            state = state_prime\n",
    "\n",
    "            if terminated == True:\n",
    "                break\n",
    "   \n",
    "    return Q, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.2298209  0.24363756 0.23384531 0.21912067]\n",
      " [0.25707458 0.25717107 0.27115917 0.24513052]\n",
      " [0.31764709 0.17997296 0.2567811  0.16904549]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.24452327 0.29665319 0.25719722 0.2517203 ]\n",
      " [0.28467607 0.36426255 0.30449671 0.28361284]\n",
      " [0.35832222 0.43245626 0.39628357 0.33115466]\n",
      " [0.33660641 0.49708142 0.39700551 0.30830871]\n",
      " [0.20764456 0.25165787 0.23332769 0.33296811]\n",
      " [0.32238629 0.41231061 0.4702718  0.42919328]\n",
      " [0.44307683 0.61719416 0.52821817 0.45591791]\n",
      " [0.70950153 0.62622125 0.59645403 0.50638969]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.29130454 0.204852   0.63171146 0.44732433]\n",
      " [0.53537175 0.76965668 0.7833215  0.71158919]\n",
      " [0.         0.         0.         0.        ]]\n",
      "[1 2 0 0 1 1 1 1 3 2 1 0 0 2 2 0]\n"
     ]
    }
   ],
   "source": [
    "# Example usage with FrozenLake environment\n",
    "Q, policy = q_learning(env, episodes=10000, alpha=0.1, gamma=0.9, epsilon=0.3) # what happens when you use a very small epsilon?\n",
    "print(Q)\n",
    "print(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Evaluate Q-learning algorithm\n",
    "\n",
    "Create a function that evaluates the Q-learning algorithm by running multiple episodes and calculating the average return. You can also store the frames of the episodes to visualize the agent's behavior if rendering is enabled. Be sure that the environment used has the `render` mode set as `rgb_array` to be able to store the frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episode(env, policy, render=False): # choose an action following a deterministic policy not an epsilon greedy policy\n",
    "    frames = []\n",
    "    obs, _ = env.reset()\n",
    "\n",
    "    if render == True:\n",
    "        frames.append(env.render())\n",
    "\n",
    "    state = obs\n",
    "    total_reward = 0\n",
    "    while True:\n",
    "        state, reward, terminated, truncated, info = env.step(policy[state])\n",
    "        total_reward += reward\n",
    "        if render == True:\n",
    "            frames.append(env.render())\n",
    "\n",
    "        if reward == 1 or terminated == True or truncated == True:\n",
    "            break\n",
    "\n",
    "    return total_reward, frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(env, policy, episodes=100):\n",
    "    total_sum = 0\n",
    "    for i in range(episodes):\n",
    "        total_reward, _ = generate_episode(env, policy, render=False)\n",
    "        total_sum += total_reward\n",
    "    return total_sum / episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average performance: 1.0\n"
     ]
    }
   ],
   "source": [
    "performance = evaluate_policy(env, policy, episodes=100)\n",
    "print(f\"Average performance: {performance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create gif of the episode\n",
    "import imageio\n",
    "\n",
    "rw, frames = generate_episode(env, policy, render=True)\n",
    "imageio.mimsave('frozenlake_Q_learning.gif', frames, loop=0, duration=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: SARSA Algorithm\n",
    "\n",
    "Implement the SARSA algorithm to learn the optimal Q-values for the FrozenLake environment. The SARSA algorithm learns the optimal Q-values by sampling episodes and updating the Q-values based on the returns obtained. Check the changes with respect the Q-learning algorithm in the slides.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa(env, episodes, alpha=0.1, gamma=0.9, epsilon=0.1):\n",
    "    \"\"\"\n",
    "    SARSA algorithm implementation.\n",
    "    Args:\n",
    "        env: The environment\n",
    "        episodes: The number of episodes to train for\n",
    "        alpha: The learning rate\n",
    "        gamma: The discount factor\n",
    "        epsilon: The exploration rate\n",
    "    Returns:\n",
    "        Q: The learned Q-table\n",
    "        policy: The learned policy\n",
    "    \"\"\"\n",
    "    Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "    policy = np.zeros(env.observation_space.n, dtype=int)\n",
    "    for i in range(episodes):\n",
    "        obs, _ = env.reset()\n",
    "        state = obs\n",
    "        action = epsilon_greedy_policy(Q, state, epsilon, env.action_space.n)\n",
    "        while True:\n",
    "            state_prime, reward, terminated, truncated, info = env.step(action)\n",
    "            action_prime = epsilon_greedy_policy(Q, state_prime, epsilon, env.action_space.n)\n",
    "            Q[state, action] = Q[state, action] + alpha * (reward + gamma * Q[state_prime, action_prime] - Q[state, action])\n",
    "            state = state_prime\n",
    "            action = action_prime\n",
    "\n",
    "            if terminated == True:\n",
    "                break\n",
    "\n",
    "    \n",
    "    for s in range(env.observation_space.n):\n",
    "        policy[s] = np.argmax(Q[s, :])\n",
    "\n",
    "    return Q, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.1011756  0.10130941 0.10390745 0.10056469]\n",
      " [0.11957302 0.11700672 0.12771754 0.10766697]\n",
      " [0.12470073 0.09949858 0.09453772 0.09591144]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.11084486 0.12389547 0.10847952 0.11205749]\n",
      " [0.14903019 0.14586025 0.18040888 0.1270721 ]\n",
      " [0.15362477 0.19363816 0.23331723 0.17886949]\n",
      " [0.19818157 0.3550716  0.10740192 0.12514775]\n",
      " [0.08491797 0.0936215  0.11794369 0.14702064]\n",
      " [0.21229231 0.20894064 0.2921035  0.15876418]\n",
      " [0.27910795 0.36634349 0.37165827 0.29655817]\n",
      " [0.40342153 0.60633318 0.42396599 0.41011681]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.14458172 0.1535057  0.32623544 0.27157872]\n",
      " [0.37596138 0.54732197 0.46660399 0.51058272]\n",
      " [0.         0.         0.         0.        ]]\n",
      "[2 2 0 0 1 2 2 1 3 2 2 1 0 2 1 0]\n"
     ]
    }
   ],
   "source": [
    "# Example usage with FrozenLake environment\n",
    "Q, policy = sarsa(env, episodes=10000, alpha=0.1, gamma=0.9, epsilon=0.3)\n",
    "print(Q)\n",
    "print(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average performance: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the learned policy using the previously defined function\n",
    "performance = evaluate_policy(env, policy, episodes=100)\n",
    "print(f\"Average performance: {performance}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ifros_autonomous_system",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
