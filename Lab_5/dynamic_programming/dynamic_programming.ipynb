{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB1: Dynamic Programming\n",
    "\n",
    "This lab will ask you to implement the following dynamic programming algorithms:\n",
    "\n",
    "* **Iterative Value Policy Evaluation**: to evaluate a given policy knowing the environment model.\n",
    "* **Policy Iteration**: to find the optimal policy for a given environment model.\n",
    "* **Value Iteration**: another approach to find the optimal policy for a given environment model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map Environment Overview\n",
    "\n",
    "To test the algorithms you need to implement you will need a *stochastic* environment from which we new its state transition model. \n",
    "\n",
    "The `MapEnv` class provides a customizable environment for Reinforcement Learning (RL) algorithms, specifically designed to simulate a grid-like world where an agent can navigate. This environment is structured similarly to the Gymnasium interface, making it compatible with various RL techniques. The key features of this environment include state representation, action management, and a stochastic option for simulating randomness in actions.\n",
    "\n",
    "The map structure is a square grid where the agent can move in four directions: down, up, right, and left. The agent's goal is to reach a specific location on the grid while avoiding obstacles, that are randomly placed in the environment. Free space is indicated as `0` and obstacles as `1` in the `map`attribute. The goal state is stored in the `goal_id` attibute (`goal` attribute also contains the goal but as a (x, y) tuple instead of a single value).\n",
    "\n",
    "The following methods are available in the `MapEnv` class:\n",
    "\n",
    "### 1. **Initialization (`__init__`)**\n",
    "- **Purpose**: Initializes the environment with a specified size, obstacle density, and stochastic behavior.\n",
    "- **Parameters**:\n",
    "  - `size`: The dimension of the square grid (default is 4).\n",
    "  - `obstacles_percent`: The proportion of obstacles to generate in the environment (default is 0.5).\n",
    "  - `stochastic`: A boolean indicating if the actions should be stochastic (default is `False`).\n",
    "\n",
    "### 2. **Plotting the Environment (`plot`)**\n",
    "- **Purpose**: Visualizes the current state of the environment, including the agent's position, the goal, and obstacles.\n",
    "- **Output**: A printed representation of the grid with symbols indicating the agent's current position ('O'), the goal ('★'), and obstacles ('■').\n",
    "\n",
    "### 3. **State Representation (`state`)**\n",
    "- **Purpose**: Returns a unique integer representing the current position of the agent in the grid.\n",
    "- **Output**: An integer that maps the agent's coordinates to a single state value.\n",
    "\n",
    "### 4. **Action Execution (`action`)**\n",
    "- **Purpose**: Updates the agent's position based on the selected action while calculating the associated reward.\n",
    "- **Parameters**: \n",
    "  - `a`: The action to be performed, represented as an integer. Available actions are: `down: 0`, `up: 1`, `right: 2`, and `left: 3`.\n",
    "- **Output**: A tuple containing the reward for the action and a boolean indicating if the goal has been reached.\n",
    "\n",
    "### 5. **Stochastic Action Execution (`sthocastic_action`)**\n",
    "- **Purpose**: Simulates a stochastic action where there is a probability of taking a random action instead of the intended one. In this environment the probability of taking the intended action is 0.85, and the probability of taking any of the other 3 actions is 0.15.\n",
    "- **Parameters**:\n",
    "  - `a`: The intended action.\n",
    "- **Output**: The result of executing the action, accounting for stochastic behavior.\n",
    "\n",
    "### 6. **Model Dynamics (`model`)**\n",
    "- **Purpose**: Provides a model of the environment, returning possible next states, rewards, and probabilities of transitioning from the current state given a specific action.\n",
    "- **Parameters**:\n",
    "  - `state`: The current state of the agent.\n",
    "  - `action`: The action the agents wants to take.\n",
    "- **Output**: A tuple containing lists of next states, rewards, and transition probabilities.\n",
    "\n",
    "### 7. **Taking a Step (`step`)**\n",
    "- **Purpose**: Executes a given action and returns the current state, reward, and whether the goal has been reached. Follows a similar interface than the [Gymnasium](https://gymnasium.farama.org/index.html) environments.\n",
    "- **Parameters**: \n",
    "  - `a`: The action to be executed.\n",
    "- **Output**: A tuple consisting of the new state, the reward, and a boolean indicating completion.\n",
    "\n",
    "### 8. **Resetting the Environment (`reset`)**\n",
    "- **Purpose**: Resets the agent's position to the starting state.\n",
    "- **Output**: The initial state of the environment.\n",
    "\n",
    "### 9. **Check if state is valid (`is_valid`)**\n",
    "- **Purpose**: Check is state `s` is free or contains an obstacle.\n",
    "- **Parameters**:\n",
    "  - `s`: The state to be checked.\n",
    "- **Output**: `True` if the state is free, `False` if it contains an obstacle.\n",
    "\n",
    "\n",
    "You can know the number of states and actions of the environment by accessing the `n_states` and `n_actions` attributes of the environment object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Get familiarized with the given environment\n",
    "\n",
    "In this exercise, you will get familiarized with the `MapEnv` environment. You will create an instance of the environment and interact with it to understand its functionalities. You will also visualize the environment to see the agent's position, the goal, and the obstacles.\n",
    "Try to creeate an instance of the environment with different parameters and observe the changes in the environment. Once experienced, define the environment with the following parameters:\n",
    "- `size`: 4\n",
    "- `obstacles_percent`: 0.4\n",
    "- `stochastic`: True\n",
    "\n",
    "Be sure that there is a path from the agent initial position (state = 0 which belongs to cell (0, 0)) to the goal indicated as a star (★) using only the four actions: down, up, right, and left. Once all the algorithms works for the 4x4 environment, you can try with bigger environments (10x10, 20x20, etc.) to see how the algorithms scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['O' ' ' '■' '★']\n",
      " [' ' ' ' ' ' ' ']\n",
      " [' ' '■' '■' ' ']\n",
      " [' ' ' ' '■' '■']] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from map_env import MapEnv\n",
    "import numpy as np\n",
    "\n",
    "env = MapEnv(size=4, obstacles_percent=0.4, stochastic=True)\n",
    "env.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.1: Implement the value iteration algorithm\n",
    "\n",
    "Implementing the value iteration algorithm, defined as follows:\n",
    "\n",
    "**Iterative Value Policy Evaluation Algorithm**\n",
    "\n",
    "1. **Initialize** $V(s) = 0 $ for all $ s \\in S $.\n",
    "2. **Repeat**:\n",
    "   - Set $\\Delta \\leftarrow 0 $.\n",
    "   - For each $s \\in S $:\n",
    "     - Set $v \\leftarrow V(s)$.\n",
    "     - Update $V(s)$ to the maximum of the sum:\n",
    "       $\n",
    "       V(s) \\leftarrow \\max_a \\sum_{s'} P_{ss'}^{\\pi(s)} [R(s') + \\gamma V(s')]\n",
    "       $\n",
    "     - Update $ \\Delta $ to the maximum of $ \\Delta $ and the absolute difference:\n",
    "       $\n",
    "       \\Delta \\leftarrow \\max(\\Delta, |v - V(s)|)\n",
    "       $\n",
    "3. **Until** $ \\Delta < \\theta $ (a small threshold)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterative_value_policy_evaluation(env: object, policy: np.ndarray, gamma: float=0.9, theta: float=0.1) -> np.ndarray:\n",
    "    V = np.zeros(shape=(1, env.n_states))\n",
    "    delta = np.inf\n",
    "    while delta >= theta:\n",
    "        delta = 0\n",
    "        for i in range(env.n_states):\n",
    "            v = V[i]\n",
    "            for action in policy:\n",
    "                next_states, rewards, probability_trans = env.model(i, action)\n",
    "                for j in range(len(next_states)):\n",
    "                    V[i] += probability_trans[j] * (rewards[j] + gamma * V[next_states[j]])\n",
    "            delta = np.maximum(delta, np.abs(v - V[i]))\n",
    "    return V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.2: Test the iterative_value_policy_evaluation function\n",
    "\n",
    "Create a random policy for the generated environment and use the implemented `iterative_value_policy_evaluation` algorithm to find out the value function $V$. You can visualize the policy and the value function $V$ using the auxiliar functions defined in `rl_auc.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['➜' '↑' '■' '★']\n",
      " ['↓' '↑' '➜' '←']\n",
      " ['←' '■' '■' '↑']\n",
      " ['↑' '←' '■' '■']] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from rl_aux import print_policy\n",
    "\n",
    "# Create a random policy\n",
    "policy = np.random.randint(0, env.n_actions, size=(env.n_states,))\n",
    "print_policy(env, policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.05, 0.05, 0.85, 0.05]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 4 is out of bounds for axis 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrl_aux\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m print_value_function\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Compute and plot the value function\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m V = \u001b[43miterative_value_policy_evaluation\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtheta\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m print_value_function(env, V)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36miterative_value_policy_evaluation\u001b[39m\u001b[34m(env, policy, gamma, theta)\u001b[39m\n\u001b[32m     10\u001b[39m             \u001b[38;5;28mprint\u001b[39m(probability_trans)\n\u001b[32m     11\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(next_states)):\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m                 V[i] += probability_trans[j] * (rewards[j] + gamma * \u001b[43mV\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnext_states\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[32m     13\u001b[39m         delta = np.maximum(delta, np.abs(v - V[i]))\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m V\n",
      "\u001b[31mIndexError\u001b[39m: index 4 is out of bounds for axis 0 with size 1"
     ]
    }
   ],
   "source": [
    "from rl_aux import print_value_function\n",
    "\n",
    "# Compute and plot the value function\n",
    "V = iterative_value_policy_evaluation(env, policy, gamma=0.9, theta=0.1)\n",
    "print_value_function(env, V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.1: \n",
    "Implement the Policy Iteration algorithm defined as follows:\n",
    "\n",
    "**Policy Iteration Algorithm**\n",
    "\n",
    "1. **Initialize** $ \\pi, \\forall s \\in S $ to a random action $ a \\in A(s) $, arbitrarily.\n",
    "2. **Repeat**:\n",
    "   - Set $ \\pi' \\leftarrow \\pi $.\n",
    "   - Compute $ V^{\\pi} $ for all states using a policy evaluation method.\n",
    "   - For each state $ s $:\n",
    "     - Update $ \\pi(s) $ to:\n",
    "       $\\pi(s) \\leftarrow \\arg \\max_{a \\in A} \\sum_{s'} P_{ss'}^{a} [R(s') + \\gamma V^{\\pi}(s')]$\n",
    "3. **Until** $ \\pi(s) = \\pi'(s) \\, \\forall s $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy iteration\n",
    "def policy_iteration(env:object, gamma: float=0.9, theta: float=0.1):\n",
    "    ... # Your code here\n",
    "    return policy, V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.2: Test the policy_iteration function\n",
    "\n",
    "Use the implemented `policy_iteration` algorithm to find the optimal policy for the generated environment. You can visualize the policy and the value function $V$ using the auxiliar functions defined in `rl_aux.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test policy iteration algorithm\n",
    "... # Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.1: Implement the Value Iteration algorithm\n",
    "\n",
    "Implement the Value Iteration algorithm defined as follows:\n",
    "\n",
    "**Iterative Value Function Evaluation Algorithm**\n",
    "\n",
    "1. **Initialize** $ V(s) $ for all $ s \\in S $ arbitrarily (for instance to 0).\n",
    "2. **Repeat**:\n",
    "   - Set $ \\Delta \\leftarrow 0 $.\n",
    "   - For each $ s \\in S $:\n",
    "     - Set $ v \\leftarrow V(s) $.\n",
    "     - Update $ V(s) $ to:\n",
    "       $\n",
    "       V(s) \\leftarrow \\max_{a} \\sum_{s'} P_{ss'}^{a} [R(s') + \\gamma V(s')]\n",
    "       $\n",
    "     - Update $ \\pi(s) $ to:\n",
    "       $\n",
    "       \\pi(s) \\leftarrow \\arg \\max_{a \\in A} \\sum_{s'} P_{ss'}^{a} [R(s') + \\gamma V^{\\pi}(s')]\n",
    "       $\n",
    "     - Update $ \\Delta $ to:\n",
    "       $\n",
    "       \\Delta \\leftarrow \\max(\\Delta, |v - V(s)|)\n",
    "       $\n",
    "3. **Until** $ \\Delta < \\theta $ (a small threshold).\n",
    "4. **Return** deterministic policy, $ \\pi $, such that:\n",
    "   $\n",
    "   \\pi(s) = \\arg \\max_{a \\in A} \\sum_{s'} P_{ss'}^{a} [R(s') + \\gamma V^{\\pi}(s')]\n",
    "   $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value iteration\n",
    "def value_iteration(env:object, gamma: float=0.9, theta: float=0.0001):\n",
    "    ... # Your code here\n",
    "    return policy, V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.2: Test the value_iteration function\n",
    "\n",
    "Use the implemented `value_iteration` algorithm to find the optimal policy for the generated environment. You can visualize the policy and the value function $V$ using the auxiliar functions defined in `rl_aux.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test value iteration algorithm\n",
    "... # Your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ifros_autonomous_system",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
