{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Behaviour Clonning\n",
    "\n",
    "Behaviour cloning is a type of imitation learning where an agent learns to perform tasks by mimicking expert demonstrations. The goal is to train a policy that can replicate the behavior of an expert based on observed state-action pairs.\n",
    "\n",
    "We are going to use the `lunar-lander-v2` environment from OpenAI Gym as our testbed. The agent will learn to land the spacecraft looking only at few demonstrations done by an *Expert*. You can use any previously trained agent to generate these demonstrations although, for simplicity, we provide a set of pre-recorded demonstrations all of them with returns above 200 points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Expert Demonstrations\n",
    "\n",
    "Demonstrations are stored in the `demos/` folder as `.npz` files. Each file contains two arrays: `observations` and `actions`. The `observations` array contains the state information, while the `actions` array contains the corresponding *discrete* actions taken by the expert.\n",
    "\n",
    "File `\"lunar_lander_10.npz` contains 10 expert demonstrations (full episodes).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2838, 8)\n",
      "(2838,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# load dictionary from a npz file\n",
    "loaded = np.load(\"datasets/lunar_lander_10.npz\")\n",
    "dataset = dict(loaded)\n",
    "print(dataset['observations'].shape)\n",
    "print(dataset['actions'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network architecture to clone the Expert\n",
    "\n",
    "Create a neural network that takes the continuous observations as input and outputs one value per discrete action available in the environment. Add also a couple of hidden layers like in the previous agents. We will use the **Cross-Entropy loss function**, which is suitable for classification problems with C classes. It computes the cross-entropy loss between the predicted *logits* and the target class. Therefore, the output layer should not include a `Softmax` activation, since the loss function internally applies it to the logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "class BCNetwork(nn.Module):\n",
    "    def __init__(self, input_dims, fc1_dims, fc2_dims, n_actions):\n",
    "        # Cretate the neural network architecture\n",
    "        ...\n",
    "\n",
    "    def forward(self, state):\n",
    "        # Define the forward pass\n",
    "        ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Behavior Cloning Class\n",
    "\n",
    "Create a `BehaviorCloning` class that encapsulates the training and evaluation logic. The class should include methods for:\n",
    "- Initializing the neural network, optimizer, and loss function.\n",
    "- Shuffling and batching the expert demonstrations.\n",
    "- Training the model using the expert demonstrations as a regular supervised learning problem.\n",
    "- Include also a method to interrogate the trained model returning the action with highest logit for a given observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BehaviorClonning():\n",
    "    def __init__(self, gamma, input_dims, n_actions, batch_size, epochs, dataset, lr=0.003, hidden_layers=64):\n",
    "        # Save all parameters ...\n",
    "        ...\n",
    "        # Initialize the policy network as a BCNetwork, \n",
    "        ...\n",
    "        # the optimizer and the Cross Entropy Loss function ...\n",
    "        ...\n",
    "\n",
    "    def shuffle_dataset(self):\n",
    "        # Shuffle the dataset ...\n",
    "        ...\n",
    "\n",
    "    def learn(self):\n",
    "        for e in range(self.epochs):\n",
    "            # Shuffle dataset at the start of each epoch ...\n",
    "            for i in range(0, self.dataset['observations'].shape[0], self.batch_size):\n",
    "                # take a batch of observations and actions and transform them to the tensors in the appropiated device ...\n",
    "                ...\n",
    "                # Training step ... (zero grad, forward pass, compute loss, backward pass, optimizer step) ...\n",
    "                ...\n",
    "                # keep track of the loss to print it later ...\n",
    "                ...\n",
    "    def save(self, path):\n",
    "        T.save(self.policy.state_dict(), path)\n",
    "    \n",
    "    def load(self, path):\n",
    "        self.policy.load_state_dict(T.load(path))\n",
    "    \n",
    "    def predict(self, obs):\n",
    "        # Prepare the observation tensor ... (transform to tensor, unsqueeze(0), to device ...)\n",
    "        ...\n",
    "        # Forward pass through the policy network without gradient calculation\n",
    "        # and return the action with the highest logit ...\n",
    "        ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Behavior Cloning Agent\n",
    "Train the behavior cloning agent using the expert demonstrations. Monitor the training loss to ensure that the model is learning effectively. Are 10 demonstrations enough to achieve good performance? How many epochs are required to reach convergence? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose hyperparameters to create the BehaviorClonning agent and \n",
    "# learn from the dataset\n",
    "agent = BehaviorClonning(gamma=..., input_dims=8, n_actions=4, \n",
    "                         batch_size=..., epochs=..., dataset=dataset)\n",
    "agent.learn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Evaluation\n",
    "\n",
    "Evaluate the trained behavior cloning agent in the `lunar-lander-v2` environment. Run multiple episodes and record the average return to assess how well the agent has learned to mimic the expert's behavior. Visualize some episodes to qualitatively evaluate the agent's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import imageio\n",
    "\n",
    "# Test the BehaviorCloning agent with the defined environment\n",
    "env = gym.make(\"LunarLander-v2\", render_mode=\"rgb_array\")\n",
    "obs = env.reset()[0]\n",
    "done = False\n",
    "total_reward = 0.0\n",
    "i = 0\n",
    "frames = [] # For storing the frames of the environment\n",
    "while not done:\n",
    "    # Predict the action using the BC agent and run it in the environment\n",
    "    ...\n",
    "    # Compute total reward and store frames for visualization\n",
    "    ...\n",
    "    total_reward += reward\n",
    "    \n",
    "print(f\"Total reward: {total_reward}\")\n",
    "env.close()\n",
    "imageio.mimsave(\"bc_lunarlander.gif\", frames, duration=0.02)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
